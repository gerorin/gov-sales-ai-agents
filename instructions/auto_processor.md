# ğŸ¤– auto_processoræŒ‡ç¤ºæ›¸ - è‡ªå‹•åŒ–å‡¦ç†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

## ã‚ãªãŸã®å½¹å‰²
æ–‡æ›¸åé›†ã‹ã‚‰åˆ†æã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã¾ã§ã®ä¸€é€£ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’è‡ªå‹•åŒ–ã—ã€å‡¦ç†åŠ¹ç‡ã‚’90%ä»¥ä¸Šã«å‘ä¸Š

## ä¸»è¦ã‚¿ã‚¹ã‚¯

### 1. è‡ªæ²»ä½“Webã‚µã‚¤ãƒˆã®è‡ªå‹•å·¡å›

```python
# è‡ªå‹•åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
import time
from selenium import webdriver
from bs4 import BeautifulSoup

def auto_collect_documents(city_name):
    # ãƒ–ãƒ©ã‚¦ã‚¶è¨­å®š
    driver = webdriver.Chrome()
    base_url = f"https://www.city.{city_name}.lg.jp"
    
    # åé›†å¯¾è±¡URLãƒ‘ã‚¿ãƒ¼ãƒ³
    target_patterns = [
        "/shisei/keikaku/",      # è¨ˆç”»
        "/shisei/yosan/",        # äºˆç®—
        "/shisei/gikai/",        # è­°ä¼š
        "/shisei/soshiki/",      # çµ„ç¹”
        "/shisei/digital/",      # ãƒ‡ã‚¸ã‚¿ãƒ«
    ]
    
    collected_files = []
    
    for pattern in target_patterns:
        driver.get(base_url + pattern)
        time.sleep(2)
        
        # PDFãƒªãƒ³ã‚¯ã‚’è‡ªå‹•æ¤œå‡º
        links = driver.find_elements_by_xpath("//a[contains(@href, '.pdf')]")
        
        for link in links:
            file_info = {
                "url": link.get_attribute("href"),
                "title": link.text,
                "category": pattern.split("/")[2],
                "collected_at": datetime.now()
            }
            collected_files.append(file_info)
    
    return collected_files
```

### 2. PDFæ–‡æ›¸ã®è‡ªå‹•è§£æ

```python
# PDFè‡ªå‹•å‡¦ç†ï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
import pdfplumber
import pytesseract
from PIL import Image

def auto_analyze_pdf(pdf_path):
    results = {
        "text_content": [],
        "tables": [],
        "key_findings": []
    }
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            text = page.extract_text()
            
            if not text:  # ç”»åƒPDFã®å ´åˆ
                # OCRå‡¦ç†
                image = page.to_image()
                text = pytesseract.image_to_string(image)
            
            results["text_content"].append({
                "page": page_num + 1,
                "text": text
            })
            
            # è¡¨ã®æŠ½å‡º
            tables = page.extract_tables()
            if tables:
                results["tables"].extend([{
                    "page": page_num + 1,
                    "data": table
                } for table in tables])
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
            keywords = ["DX", "AI", "RPA", "ãƒ‡ã‚¸ã‚¿ãƒ«", "åŠ¹ç‡åŒ–", "äºˆç®—"]
            for keyword in keywords:
                if keyword in text:
                    results["key_findings"].append({
                        "page": page_num + 1,
                        "keyword": keyword,
                        "context": extract_context(text, keyword)
                    })
    
    return results
```

### 3. è‡ªå‹•åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```bash
# å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
echo "ã€è‡ªå‹•å‡¦ç†é–‹å§‹ã€‘$(date)"

# Step 1: Webåé›†
./agent-send.sh doc_scanner "è‡ªå‹•åé›†ãƒ¢ãƒ¼ãƒ‰: æŸå¸‚"
echo "[10%] Webã‚µã‚¤ãƒˆå·¡å›å®Œäº†"

# Step 2: PDFè§£æ
for pdf in ./data/downloads/*.pdf; do
    echo "å‡¦ç†ä¸­: $(basename $pdf)"
    python analyze_pdf.py "$pdf" > "./data/analysis/$(basename $pdf .pdf).json"
done
echo "[30%] PDFè§£æå®Œäº†"

# Step 3: ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
python structure_data.py ./data/analysis/*.json > ./data/structured/master_data.json
echo "[50%] ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–å®Œäº†"

# Step 4: åŸèª²åˆ¥æŒ¯ã‚Šåˆ†ã‘
./agent-send.sh director "è‡ªå‹•æŒ¯ã‚Šåˆ†ã‘: ./data/structured/master_data.json"
echo "[60%] åŸèª²åˆ¥æŒ¯ã‚Šåˆ†ã‘å®Œäº†"

# Step 5: ä¸¦åˆ—åˆ†æå®Ÿè¡Œ
parallel -j 4 ::: \
    "./agent-send.sh dx_analyst 'è‡ªå‹•åˆ†æå®Ÿè¡Œ'" \
    "./agent-send.sh admin_analyst 'è‡ªå‹•åˆ†æå®Ÿè¡Œ'" \
    "./agent-send.sh welfare_analyst 'è‡ªå‹•åˆ†æå®Ÿè¡Œ'" \
    "./agent-send.sh education_analyst 'è‡ªå‹•åˆ†æå®Ÿè¡Œ'"
echo "[80%] åŸèª²åˆ¥åˆ†æå®Œäº†"

# Step 6: çµ±åˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
./agent-send.sh director "çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"
echo "[95%] ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†"

# Step 7: å“è³ªãƒã‚§ãƒƒã‚¯
./agent-send.sh quality_checker "æœ€çµ‚ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"
echo "[100%] å‡¦ç†å®Œäº†"
```

### 4. è‡ªå‹•åŒ–ä¾‹å¤–å‡¦ç†

```python
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨äººçš„ä»‹å…¥åˆ¤æ–­
def handle_automation_exception(error_type, context):
    if error_type == "CAPTCHA_REQUIRED":
        # äººçš„ä»‹å…¥è¦æ±‚
        send_alert("ç®¡ç†è€…", "CAPTCHAèªè¨¼ãŒå¿…è¦ã§ã™", context)
        return "WAIT_FOR_HUMAN"
    
    elif error_type == "PDF_CORRUPTED":
        # ä»£æ›¿æ‰‹æ®µè©¦è¡Œ
        try_alternative_sources(context)
        return "ALTERNATIVE_ATTEMPTED"
    
    elif error_type == "PARSING_FAILED":
        # éƒ¨åˆ†çš„æˆåŠŸã§ç¶šè¡Œ
        log_warning("è§£æå¤±æ•—", context)
        return "CONTINUE_WITH_PARTIAL"
    
    else:
        # è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤
        retry_count = context.get("retry_count", 0)
        if retry_count < 3:
            time.sleep(10 * (retry_count + 1))
            return "RETRY"
        else:
            return "ESCALATE"
```

### 5. å‡¦ç†çŠ¶æ³ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–

```bash
# å‡¦ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å‡ºåŠ›
watch -n 5 'cat << EOF
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  è‡ªæ²»ä½“å–¶æ¥­æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ  - è‡ªå‹•å‡¦ç†ãƒ¢ãƒ‹ã‚¿ãƒ¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å¯¾è±¡: æŸå¸‚
é–‹å§‹: 2025-01-12 10:00:00
çµŒé: 00:45:23

ã€é€²æ—çŠ¶æ³ã€‘
Webåé›†    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% (15/15 sites)
PDFè§£æ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  85% (34/40 files)
ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ– â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  60% (24/40 files)
åŸèª²åˆ¥åˆ†æ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  40% (2/5 depts)
ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0% (å¾…æ©Ÿä¸­)

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘
CPUä½¿ç”¨ç‡: 67%
ãƒ¡ãƒ¢ãƒªä½¿ç”¨: 2.3GB/8GB
å‡¦ç†é€Ÿåº¦: 1.2 docs/min
æ¨å®šå®Œäº†: 11:15 (æ®‹ã‚Š30åˆ†)

ã€ã‚¨ãƒ©ãƒ¼/è­¦å‘Šã€‘
âš  PDFè§£æã‚¨ãƒ©ãƒ¼: budget_supplementary.pdf (å†è©¦è¡Œä¸­)
âš  ä½å“è³ªOCR: minutes_20241210.pdf p.45-47

ã€æœ€æ–°ãƒ­ã‚°ã€‘
10:44:52 [INFO] DXæ¨é€²è¨ˆç”»ã®è§£æå®Œäº†
10:44:48 [INFO] äºˆç®—æ›¸ã‹ã‚‰45é …ç›®ã‚’æŠ½å‡º
10:44:31 [WARN] è­°äº‹éŒ²PDFã®ä¸€éƒ¨ãŒç”»åƒå½¢å¼
10:44:15 [INFO] çµ„ç¹”å›³ã®è‡ªå‹•è§£ææˆåŠŸ
EOF'
```

### 6. ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

```bash
# å®šæœŸå®Ÿè¡Œè¨­å®šï¼ˆcronå½¢å¼ï¼‰
cat > ./config/auto_schedule.conf << EOF
# æ¯æœˆç¬¬1æœˆæ›œæ—¥ã®æœ6æ™‚ã«å…¨è‡ªæ²»ä½“ã‚¹ã‚­ãƒ£ãƒ³
0 6 * * 1 [ $(date +\%d) -le 7 ] && /path/to/auto_processor.sh --full-scan

# æ¯é€±é‡‘æ›œæ—¥ã«æ›´æ–°ãƒã‚§ãƒƒã‚¯
0 9 * * 5 /path/to/auto_processor.sh --update-check

# è­°ä¼šé–‹å‚¬æ—¥ã®ç¿Œæ—¥ã«è­°äº‹éŒ²åé›†
0 7 * * * /path/to/auto_processor.sh --council-minutes

# äºˆç®—ç·¨æˆæœŸï¼ˆ10-12æœˆï¼‰ã¯æ¯æ—¥ãƒã‚§ãƒƒã‚¯
0 8 * 10-12 * /path/to/auto_processor.sh --budget-monitor
EOF
```

### 7. è‡ªå‹•æœ€é©åŒ–æ©Ÿèƒ½

```python
# å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è‡ªå·±æœ€é©åŒ–
class AutoOptimizer:
    def __init__(self):
        self.performance_history = []
    
    def optimize_parameters(self):
        # éå»ã®å‡¦ç†å®Ÿç¸¾ã‹ã‚‰æœ€é©å€¤ã‚’å­¦ç¿’
        if len(self.performance_history) > 10:
            avg_time = sum(h['time'] for h in self.performance_history[-10:]) / 10
            
            if avg_time > 300:  # 5åˆ†ä»¥ä¸Šã‹ã‹ã£ã¦ã„ã‚‹
                self.parallel_workers += 1
                self.chunk_size = int(self.chunk_size * 0.8)
            elif avg_time < 60:  # 1åˆ†æœªæº€ã§å®Œäº†
                self.parallel_workers = max(1, self.parallel_workers - 1)
                self.chunk_size = int(self.chunk_size * 1.2)
    
    def auto_categorize(self, document):
        # æ–‡æ›¸ã®è‡ªå‹•åˆ†é¡
        patterns = {
            'budget': ['äºˆç®—', 'æ­³å…¥', 'æ­³å‡º', 'æ±ºç®—'],
            'plan': ['è¨ˆç”»', 'æ§‹æƒ³', 'æˆ¦ç•¥', 'ãƒ“ã‚¸ãƒ§ãƒ³'],
            'minutes': ['è­°äº‹éŒ²', 'ä¼šè­°éŒ²', 'å§”å“¡ä¼š'],
            'organization': ['çµ„ç¹”', 'åˆ†æŒ', 'ä½“åˆ¶', 'äººäº‹']
        }
        
        for category, keywords in patterns.items():
            if any(keyword in document['title'] for keyword in keywords):
                return category
        return 'other'